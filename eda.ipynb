{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy.stats import f_oneway\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, learning_curve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from scipy import stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_path = \"data/agri.db\"  \n",
    "conn = sqlite3.connect(db_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show all tables in the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"SELECT * FROM farm_data;\"\n",
    "df = pd.read_sql(query, conn)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Preparing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning and preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_ppm = [\"Nutrient N Sensor (ppm)\", \"Nutrient P Sensor (ppm)\", \"Nutrient K Sensor (ppm)\"]\n",
    "\n",
    "for col in remove_ppm:\n",
    "    df[col] = (\n",
    "        df[col]\n",
    "        .astype(str)  # Convert all values to string\n",
    "        .str.replace(\"ppm\", \"\", regex=False)  # Remove 'ppm'\n",
    "        .str.strip()  # Remove whitespace\n",
    "        .replace([\"\", \"None\", \"nan\"], pd.NA)  # Replace empty strings and 'None' with Pandas NA\n",
    "        .astype(\"Int64\")  # Convert to integer while keeping NaNs\n",
    "    )\n",
    "\n",
    "df.dtypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_columns = [\"System Location Code\",\"Previous Cycle Plant Type\",\"Plant Type\", \"Plant Stage\"]\n",
    "\n",
    "for col in object_columns:\n",
    "    df[col] = df[col].astype(str).str.strip().str.title()\n",
    "    df[f\"new_{col}\"] = pd.factorize(df[col])[0]  # Assign unique integer values\n",
    "\n",
    "df[[f\"new_{col}\" for col in object_columns]] = df[[f\"new_{col}\" for col in object_columns]].astype(\"int64\")\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change object type into int type for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df.drop(columns=object_columns)\n",
    "df_new.dtypes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking for missing values percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df_new.columns:\n",
    "    missing_percentage = (df_new[col].isnull().sum() / len(df_new)) * 100\n",
    "    print(f\"Percentage of missing values in '{col}': {missing_percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that column \"Humidity Sensor (%)\" has a missing value percentage of 67.61% which is quite high. The column might be irrelvant for analysis.\n",
    "For columns like \"Temperature Sensor (%)\", \"Light Intensity Sensor (lux)\", \"Nutrient N Sensor (ppm)\", \"Nutrient K Sensor (ppm)\", \" Nutrient P Sensor (ppm)\" and Water Levl Sensor (mm)\", we can either fill in with estimated values or removed the rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check correlation between Humidity Sensor and Temperature Sensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_temp_humidity = df_new['Temperature Sensor (°C)'].corr(df['Humidity Sensor (%)'])\n",
    "print(f\"Correlation (Temp vs Humidity): {corr_temp_humidity:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check correlation between Humidity Sensor and Plant-Type-Stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine \"Plant Type\" and \"Plant Stage\" into a new feature\n",
    "df_new[\"Plant Type-Stage\"] = df_new[\"new_Plant Type\"] * 10 + df_new[\"new_Plant Stage\"]  # Unique encoding\n",
    "\n",
    "# Compute correlation\n",
    "correlation = df_new[[\"Humidity Sensor (%)\", \"Plant Type-Stage\"]].corr()\n",
    "\n",
    "# Print correlation matrix\n",
    "print(\"Correlation between Humidity Sensor (%) and Plant Type-Stage:\")\n",
    "print(correlation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that humidity has low correalation with Temperature Sensor and Plant Type/Plant Stage, hence we can drop the coloumn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df_new.drop(columns=\"Humidity Sensor (%)\")\n",
    "\n",
    "print(df_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns = df_new.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "# Calculate correlations with Temperature Sensor using only numeric columns\n",
    "correlations = df_new[numeric_columns].corr()['Temperature Sensor (°C)'].sort_values(ascending=False)\n",
    "\n",
    "# Print the correlations\n",
    "print(correlations)\n",
    "\n",
    "# Create a visualization using only numeric columns\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(df_new[numeric_columns].corr(), annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.title('Correlation Matrix (Numeric Columns Only)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Bar chart for Temperature correlations\n",
    "plt.figure(figsize=(12, 6))\n",
    "correlations.drop('Temperature Sensor (°C)', errors='ignore').plot(kind='bar')\n",
    "plt.title('Correlation with Temperature Sensor')\n",
    "plt.ylabel('Correlation Coefficient')\n",
    "plt.axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will now fix the missing values of the data and test which method will increase the correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df_new.columns:\n",
    "    missing_percentage = (df_new[col].isnull().sum() / len(df_new)) * 100\n",
    "    print(f\"Percentage of missing values in '{col}': {missing_percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_up = [\"Temperature Sensor (°C)\", \"Light Intensity Sensor (lux)\", \"Nutrient N Sensor (ppm)\", \"Nutrient P Sensor (ppm)\", \"Nutrient K Sensor (ppm)\", \"Water Level Sensor (mm)\"]\n",
    "\n",
    "df_filled = df_new.copy()  # Use parentheses to call the copy method\n",
    "\n",
    "# Initialize the KNNImputer\n",
    "knn_imputer = KNNImputer(n_neighbors=5)  # Uses 5 nearest neighbors\n",
    "\n",
    "# Apply KNN imputation only to the selected columns in the new DataFrame\n",
    "df_filled[fill_up] = knn_imputer.fit_transform(df_filled[fill_up])\n",
    "\n",
    "# Print the new DataFrame\n",
    "print(df_filled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if all columns are filled\n",
    "\n",
    "for col in df_filled.columns:\n",
    "    missing_percentage = (df_filled[col].isnull().sum() / len(df_filled)) * 100\n",
    "    print(f\"Percentage of missing values in '{col}': {missing_percentage:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlations with Temperature Sensor using only numeric columns\n",
    "correlations = df_filled[numeric_columns].corr()['Temperature Sensor (°C)'].sort_values(ascending=False)\n",
    "\n",
    "# Print the correlations\n",
    "print(correlations)\n",
    "\n",
    "# Create a visualization using only numeric columns\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(df_filled[numeric_columns].corr(), annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.title('Correlation Matrix (Numeric Columns Only)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Bar chart for Temperature correlations\n",
    "plt.figure(figsize=(12, 6))\n",
    "correlations.drop('Temperature Sensor (°C)', errors='ignore').plot(kind='bar')\n",
    "plt.title('Correlation with Temperature Sensor')\n",
    "plt.ylabel('Correlation Coefficient')\n",
    "plt.axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that by filling up the rows with missing valuers, the correlation actually decreased. Hence, I will check if removing rows with the missing values would improve the correlation instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df_new.dropna()\n",
    "\n",
    "for col in df_new.columns:\n",
    "    missing_percentage = (df_new[col].isnull().sum() / len(df_new)) * 100\n",
    "    print(f\"Percentage of missing values in '{col}': {missing_percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations = df_new[numeric_columns].corr()['Temperature Sensor (°C)'].sort_values(ascending=False)\n",
    "\n",
    "# Print the correlations\n",
    "print(correlations)\n",
    "\n",
    "# Create a visualization using only numeric columns\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(df_new[numeric_columns].corr(), annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.title('Correlation Matrix (Numeric Columns Only)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Bar chart for Temperature correlations\n",
    "plt.figure(figsize=(12, 6))\n",
    "correlations.drop('Temperature Sensor (°C)', errors='ignore').plot(kind='bar')\n",
    "plt.title('Correlation with Temperature Sensor')\n",
    "plt.ylabel('Correlation Coefficient')\n",
    "plt.axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the correlation improved slightly. Hence, I will be removing rows with missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I will be using different models and test which model performs better then creating a predictive model for temperature condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Prepare features and target\n",
    "X = df_new.drop('Temperature Sensor (°C)', axis=1)  # All other columns as features\n",
    "y = df_new['Temperature Sensor (°C)']               # Temperature as target\n",
    "\n",
    "# 2. Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 3. Scale the features (important for many ML algorithms)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Ridge Regression': Ridge(alpha=1.0),\n",
    "    'Lasso Regression': Lasso(alpha=0.1),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "\n",
    "results = {}\n",
    "feature_importances = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    # Train the model\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "    # Evaluate\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100  # MAPE in percentage\n",
    "    \n",
    "    results[name] = {'MSE': mse, 'RMSE': rmse, 'MAE': mae, 'R2': r2, 'MAPE': mape}\n",
    "    \n",
    "    # Store feature importances if available\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        feature_importances[name] = pd.Series(\n",
    "            model.feature_importances_,\n",
    "            index=X.columns\n",
    "        ).sort_values(ascending=False)\n",
    "    elif hasattr(model, 'coef_'):\n",
    "        feature_importances[name] = pd.Series(\n",
    "            np.abs(model.coef_),\n",
    "            index=X.columns\n",
    "        ).sort_values(ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model Performance:\")\n",
    "for name, metrics in results.items():\n",
    "    print(f\"{name}: MSE = {metrics['MSE']:.4f}, RMSE = {metrics['RMSE']:.4f}, MAE = {metrics['MAE']:.4f}, R2 = {metrics['R2']:.4f}, MAPE = {metrics['MAPE']:.2f}%\")\n",
    "\n",
    "# Identify the best model based on R2 score\n",
    "best_model_name = max(results, key=lambda x: results[x]['R2'])\n",
    "print(f\"\\nBest model: {best_model_name} with R2 = {results[best_model_name]['R2']:.4f}\")\n",
    "\n",
    "# Plot feature importances if available\n",
    "if best_model_name in feature_importances:\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    feature_importances[best_model_name].plot(kind='bar', color='skyblue')\n",
    "    plt.title(f'Feature Importances - {best_model_name}')\n",
    "    plt.xlabel('Features')\n",
    "    plt.ylabel('Importance')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Cross-validation for the best model\n",
    "cv_scores = cross_val_score(best_model, X_train_scaled, y_train, cv=5, scoring='r2')\n",
    "print(f\"\\nCross-Validation R2 Scores: {cv_scores}\")\n",
    "print(f\"Mean Cross-Validation R2 Score: {cv_scores.mean():.4f}\")\n",
    "\n",
    "# Confidence interval for R2\n",
    "confidence = 0.95\n",
    "ci = stats.norm.interval(confidence, loc=cv_scores.mean(), scale=cv_scores.std() / np.sqrt(len(cv_scores)))\n",
    "print(f\"95% Confidence Interval for R2: {ci}\")\n",
    "\n",
    "# Residual analysis\n",
    "y_pred = best_model.predict(X_test_scaled)\n",
    "residuals = y_test - y_pred\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(y_pred, residuals, alpha=0.5, color='blue')\n",
    "plt.axhline(y=0, color='r', linestyle='--', lw=2)\n",
    "plt.xlabel('Predicted Temperature (°C)')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residual Plot')\n",
    "plt.show()\n",
    "\n",
    "# Predicted vs Actual plot with different colors\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(y_test, y_test, color='blue', alpha=0.5, label='Actual')  # Actual values in blue\n",
    "plt.scatter(y_test, y_pred, color='orange', alpha=0.5, label='Predicted')  # Predicted values in orange\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2, label='Ideal Prediction')  # Ideal line\n",
    "plt.xlabel('Actual Temperature (°C)')\n",
    "plt.ylabel('Predicted Temperature (°C)')\n",
    "plt.title('Predicted vs Actual Temperature')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "\n",
    "# Learning curves\n",
    "train_sizes, train_scores, test_scores = learning_curve(\n",
    "    best_model, X_train_scaled, y_train, cv=5, scoring='r2', train_sizes=np.linspace(0.1, 1.0, 10)\n",
    ")\n",
    "\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(train_sizes, train_scores_mean, 'o-', color='r', label='Training score')\n",
    "plt.plot(train_sizes, test_scores_mean, 'o-', color='g', label='Cross-validation score')\n",
    "plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color='r')\n",
    "plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1, color='g')\n",
    "plt.title('Learning Curves')\n",
    "plt.xlabel('Training Set Size')\n",
    "plt.ylabel('R2 Score')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
